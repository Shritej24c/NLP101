{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><span style=\"font-size:2em; font-weight: bold;\">Lecture 5â€”Optimization</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data science: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation\n",
    "\n",
    "**Linear formulation**\n",
    "\n",
    "$$\\mathcal L=\\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$$\n",
    "$$\\mathcal L=\\prod_{i=1}^n F(x_i'\\beta)^{y_i}(1-F(x_i'\\beta))^{1-y_i}$$\n",
    "$$F(x)=\\frac{1}{1+e^{-x}}$$\n",
    "$$\\ln\\mathcal L=\\sum_{i=1}^n y_i \\ln{F(x_i'\\beta)}+(1-y_i)\\ln{(1-F(x_i'\\beta))}$$\n",
    "$$\\ln\\mathcal L=\\left[\\ln{F(\\beta'\\mathbf X')}\\right]y+\\left[\\ln{(\\mathbf{1}'-F(\\beta'\\mathbf X'))}\\right](1-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:36:48.546583Z",
     "start_time": "2023-02-24T23:36:48.528496Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "expit = lambda x: 1/(1+np.exp(-x))\n",
    "def loglike(x,y,b):\n",
    "    Fx = expit(b.T@x.T)\n",
    "    return np.log(Fx)@y+np.log(1-Fx)@(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient**\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)}{F(\\mathbf X\\beta)}\\right)y-\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)}{\\mathbf 1-F(\\mathbf X\\beta)}\\right)(1-y)$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)(1-F(\\mathbf X\\beta))}{(1-F(\\mathbf X\\beta))F(\\mathbf X\\beta)}\\right)y-\\mathbf X'\\text{diag}\\left(\\frac{f(\\mathbf X\\beta)F(\\mathbf X\\beta)}{F(\\mathbf X\\beta)(1-F(\\mathbf X\\beta))}\\right)(1-y)$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[\\text{diag}\\left(1-F(\\mathbf X\\beta)\\right)y-\\text{diag}\\left(F(\\mathbf X\\beta)\\right)(1-y)\\right]$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[\\text{diag}\\left(y-F(\\mathbf X\\beta)y-F(\\mathbf X\\beta)+F(\\mathbf X\\beta)y)\\right)\\right]\\mathbf 1$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[\\text{diag}\\left(y-F(\\mathbf X\\beta)\\right)\\right]\\mathbf 1$$\n",
    "$$\\frac{d\\ln\\mathcal L}{d\\beta}=\\mathbf X'\\left[y-F(\\mathbf X\\beta)\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:36:49.441234Z",
     "start_time": "2023-02-24T23:36:49.425507Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gradient(x,y,b):\n",
    "    Fx = expit(x@b)\n",
    "    return x.T@(y-Fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hessian**\n",
    "$$\\frac{d}{d\\beta}\\frac{d\\ln\\mathcal L}{d\\beta}'=\\frac{d}{d\\beta}\\left[y'-F(\\beta'\\mathbf X')\\right]\\mathbf X$$\n",
    "$$\\frac{d^2\\ln\\mathcal L}{d\\beta d\\beta'}=-\\mathbf X'\\left[\\text{diag}\\left(f(\\mathbf X\\beta)\\right)\\right]\\mathbf X$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:36:50.269539Z",
     "start_time": "2023-02-24T23:36:50.255568Z"
    }
   },
   "outputs": [],
   "source": [
    "def hessian(x,y,b):\n",
    "    Fx = expit(x@b)\n",
    "    fx = Fx*(1-Fx)\n",
    "    return -x.T@np.diagflat(fx.flatten())@x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** Crammer-Rao Lower Bound\n",
    "\n",
    "Assume\n",
    "$\\mathcal{L}$ is continuous and differentiable. For any unbiased estimator $\\hat\\theta$, the variance is bounded below by\n",
    "$$\\text{Var}\\left[\\hat\\theta\\right]\\ge\\left[-\\text{E}\\left[\\frac{d^2\\ln{\\mathcal{L}}}{d\\theta d\\theta'}\\right]\\right]^{-1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming--Numerical Optimization Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search\n",
    "\n",
    "Search over a given parameter space. Check every possible option for the optimum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:36:53.162326Z",
     "start_time": "2023-02-24T23:36:52.431883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(4.004004004004004, 5.995995995995996)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "def grid_search(func,space,maximize=False):\n",
    "    vstates = [(x,func(x)) for x in space]\n",
    "    vstates.sort(key=lambda x: x[1])\n",
    "    return vstates[-1][0] if maximize else vstates[0][0]\n",
    "\n",
    "x = np.linspace(0,10,1000).tolist()\n",
    "func = lambda x: (x[0]-4.0001)**2*(x[1]-6.0001)**2\n",
    "grid_search(func,product(x,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Walk along the slope of the curve by steps proportional to the opposite of the size of the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:36:53.660780Z",
     "start_time": "2023-02-24T23:36:53.655240Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(func,gradient,init_x:np.ndarray,learning_rate:float=0.005,max_reps:int=10000,maximize=False):\n",
    "    x = init_x.copy()\n",
    "    for i in range(max_reps):\n",
    "        gx = gradient(x)\n",
    "        x0 = x.copy()\n",
    "        flast = func(x)\n",
    "        x += gx*learning_rate if maximize else -gx*learning_rate\n",
    "        if (func(x)<flast and maximize and i>2) or (func(x)>flast and (not maximize) and i>2): \n",
    "            x = x0\n",
    "            break\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method\n",
    "\n",
    "Use a zero finding algorithm on the gradient to isolate where the gradient is flat, i.e., where the maximum or minimum values of the function are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:36:54.711283Z",
     "start_time": "2023-02-24T23:36:54.702546Z"
    }
   },
   "outputs": [],
   "source": [
    "def newton(gradient,hessian,init_x:np.ndarray,max_reps:int=100,tolerance:float=1e-16):\n",
    "    x = init_x.copy()\n",
    "    for i in range(max_reps):\n",
    "        update = -np.linalg.solve(hessian(x),gradient(x))\n",
    "        x += update\n",
    "        if np.abs(update).sum()<tolerance:\n",
    "            return (x,i)\n",
    "    raise Exception('Newton did not converge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:37:40.276322Z",
     "start_time": "2023-02-24T23:37:39.809201Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cleands'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-1cc841f04c79>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mcleands\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mlogistic_regressor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlinear_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__fit__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m         \u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0miters\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__max_likelihood__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_feat\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'cleands'"
     ]
    }
   ],
   "source": [
    "from cleands import *\n",
    "\n",
    "class logistic_regressor(linear_model):\n",
    "    def __fit__(self,x,y):\n",
    "        params,self.iters = self.__max_likelihood__(np.zeros(self.n_feat))\n",
    "        return params\n",
    "    @property\n",
    "    def vcov_params(self):return self.__vcov_params_lnL__()\n",
    "    def evaluate_lnL(self,pred):return self.y.T@np.log(pred)+(1-self.y).T@np.log(1-pred)\n",
    "    def _gradient_(self,coefs):return self.x.T@(self.y-expit(self.x@coefs))\n",
    "    def _hessian_(self,coefs):\n",
    "        Fx = expit(self.x@coefs)\n",
    "        return -self.x.T@np.diagflat((Fx*(1-Fx)).values)@self.x\n",
    "    def predict(self,target):return expit(target@self.params)\n",
    "\n",
    "class LogisticRegressor(logistic_regressor,broom_model):\n",
    "    def __init__(self,x_vars:list,y_var:str,data:pd.DataFrame,*args,**kwargs):\n",
    "        super(LogisticRegressor,self).__init__(data[x_vars],data[y_var],*args,**kwargs)\n",
    "        self.x_vars = x_vars\n",
    "        self.y_var = y_var\n",
    "        self.data = data\n",
    "    def _glance_dict_(self):\n",
    "        return {'mcfadden.r.squared':self.r_squared,\n",
    "                'adjusted.r.squared':self.adjusted_r_squared,\n",
    "                'self.df':self.n_feat,\n",
    "                'resid.df':self.degrees_of_freedom,\n",
    "                'aic':self.aic,\n",
    "                'bic':self.bic,\n",
    "                'log.likelihood':self.lnL,\n",
    "                'deviance':self.deviance,\n",
    "                'resid.var':self.ssq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:37:42.804709Z",
     "start_time": "2023-02-24T23:37:42.792081Z"
    }
   },
   "outputs": [],
   "source": [
    "from cleands import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:38:26.826226Z",
     "start_time": "2023-02-24T23:38:26.805562Z"
    }
   },
   "outputs": [],
   "source": [
    "## Data generation\n",
    "df = pd.DataFrame(np.random.normal(size=(10000,4)),columns=['x1','x2','x3','y'])\n",
    "df['y'] += df[['x1','x2','x3']]@np.random.uniform(size=(3,))\n",
    "df['y'] = (df['y']>0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:38:31.129244Z",
     "start_time": "2023-02-24T23:38:30.078885Z"
    }
   },
   "outputs": [],
   "source": [
    "## Run the model\n",
    "model = LogisticRegressor(*add_intercept(['x1','x2','x3'],'y',df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:38:33.793245Z",
     "start_time": "2023-02-24T23:38:33.353653Z"
    }
   },
   "outputs": [],
   "source": [
    "## See table\n",
    "model.tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:38:56.035447Z",
     "start_time": "2023-02-24T23:38:56.013006Z"
    }
   },
   "outputs": [],
   "source": [
    "model.glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-24T23:39:06.963236Z",
     "start_time": "2023-02-24T23:39:06.958741Z"
    }
   },
   "outputs": [],
   "source": [
    "model.iters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive partitioning trees\n",
    "\n",
    "Write a class that implements a recursive partitioning algorithm. Use our common machine learning code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quaternions\n",
    "\n",
    "The Quaternions are a generalization of complex numbers. Where the complex numbers have two components, $a$ and $b$, for a number $a+bi$, the Quaternions have four parts $a, b, c$ and $d$: $$a+bi+cj+dk$$\n",
    "\n",
    "The Quaternions have four basic operations: addition, subtraction, multiplication, and the inverse. Also write a str representation function. Your job is to write a quaternion class which implements these operations. You can learn how to perform these operations on the Quaternions' wikipedia page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}