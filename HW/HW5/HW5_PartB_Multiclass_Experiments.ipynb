{"cells":[{"cell_type":"markdown","id":"59943946","metadata":{"id":"59943946"},"source":["# <font color = 'indianred'>**HW5 PartB - MultiClass Classification with different checkpoints- 6 Points** </font>\n","\n","**Homework Instructions: Model Performance Comparisons**\n","\n","---\n","\n","**Objective:**  \n","Compare the performance of different model checkpoints on a given task using the provided functions and notebook setup.\n","\n","---\n","\n","**Instructions:**\n","\n","1. **Setup & Initialization:**\n","    - Start by setting up your environment. Ensure you have all necessary dependencies installed.\n","    - All functions and required code blocks are pre-written for you.\n","\n","2. **Experiment 1 - Using DistilBERT:**\n","    - In the specified code block, set the model checkpoint name to `\"distilbert-base-uncased\"`.\n","\n","3. **Experiment 2 - Choosing a New Model:**\n","    -  Search for a model similar in size to DistilBERT that have a similar or better performance.\n","    - Get the model check point name from the [Hugging Face Model Hub](https://huggingface.co/models).\n","    - In this experiment, replace the model checkpoint name with the one you've chosen from the Model Hub.\n","\n","\n","4. **Experiment 3 - Another Model Choice:**\n","    - Go back to the [Hugging Face Model Hub](https://huggingface.co/models).\n","    - Select another model (different from your choice in Experiment 2) with a similar size to DistilBERT.\n","    - Update the model checkpoint name in the cell for experiment 3.\n","\n","5. **Conclusion:**\n","    - Analyze the results of all three experiments.\n","    - Discuss any differences in performance between the models. What might be causing these differences?\n","    - Conclude by summarizing your findings and providing insights on which model checkpoint performed best and why.\n","\n","---\n","\n","**Note:** It's essential to only change the model checkpoint name in each experiment. All other parameters and functions should remain unchanged to ensure a fair comparison between the models.\n","\n","---\n","- **You have to submit two files for this part of the HW**\n","  >(1) ipynb (colab notebook) and<br>\n","  >(2) pdf file (pdf version of the colab file).**\n","- **Files should be named as follows**:\n",">FirstName_LastName_HW_5_PartB**"]},{"cell_type":"markdown","id":"e8665b2c","metadata":{"id":"e8665b2c"},"source":["This notebook has been modified to include functions that allow switching between different pre-trained models for training and evaluation."]},{"cell_type":"markdown","id":"145419c4","metadata":{"id":"145419c4"},"source":["## Set Up Environment"]},{"cell_type":"code","execution_count":null,"id":"63fbeb4c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6840,"status":"ok","timestamp":1696573679114,"user":{"displayName":"Shaannoor Mann","userId":"02520257695567980696"},"user_tz":300},"id":"63fbeb4c","outputId":"6d62d3fa-f235-42fa-def4-f8a26c6e2dae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from pathlib import Path\n","if 'google.colab' in str(get_ipython()):\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","    !pip install datasets transformers evaluate wandb accelerate -U -qq\n","    base_folder = Path(\"/content/drive/MyDrive/data\") # CHANGE BASED ON YOUR SETUP\n","else:\n","    base_folder = Path(\"/home/harpreet/Insync/google_drive_shaannoor/data\") # CHANGE BASED ON YOUR SETUP\n","\n","\n","from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n","from transformers import AutoTokenizer, DataCollatorWithPadding, pipeline\n","from datasets import load_dataset, DatasetDict, Dataset, ClassLabel\n","import evaluate\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","import wandb\n","\n","import numpy as np\n","from sklearn.metrics import ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","import random\n","\n","import textwrap\n","import gc\n","\n","from datasets.utils.logging import disable_progress_bar\n","disable_progress_bar()\n","\n","data_folder = base_folder/'datasets/Classification_HW/csv_files' # CHANGE BASED ON YOUR SETUP\n","model_folder = base_folder/'models/nlp_fall_2023/HW5' # CHANGE BASED ON YOUR SETUP\n","model_folder.mkdir(exist_ok=True)\n"]},{"cell_type":"markdown","source":["**---------DO NOT CHANGE ANY OF THE FUNCTIONS------------**"],"metadata":{"id":"wnpoK38UK-iu"},"id":"wnpoK38UK-iu"},{"cell_type":"markdown","id":"e3b5d101","metadata":{"id":"e3b5d101"},"source":["## Function to Load Dataset"]},{"cell_type":"code","execution_count":null,"id":"188b9434","metadata":{"id":"188b9434"},"outputs":[],"source":["def load_custom_dataset(data_path, label_columns_name, text_column_name, class_names=None):\n","    from datasets import load_dataset\n","    dataset = load_dataset('csv', data_files=str(data_path))\n","    selected_columns = {\n","    'text': dataset['train'][text_column_name],\n","    'label': dataset['train'][label_columns_name]\n","    }\n","\n","    # Create a new dataset with the selected columns\n","    dataset_selected_columns = Dataset.from_dict(selected_columns)\n","    dataset_selected_columns.set_format(type='pandas')\n","    df = dataset_selected_columns[:]\n","    df['text'] = df['text'].fillna('')\n","    dataset_selected_columns.reset_format()\n","\n","    if class_names:\n","        dataset_selected_columns = dataset_selected_columns.cast_column('label', ClassLabel(names = class_names))\n","\n","    return dataset_selected_columns\n"]},{"cell_type":"markdown","id":"0a5740d9","metadata":{"id":"0a5740d9"},"source":["## Function to Split Dataset\n"]},{"cell_type":"code","execution_count":null,"id":"29a9e5fb","metadata":{"id":"29a9e5fb"},"outputs":[],"source":["def split_dataset(dataset, train_size, val_size, test_size):\n","    test_val_splits = dataset.train_test_split(train_size=train_size, seed=42, stratify_by_column='label')\n","    train_split= test_val_splits['train']\n","    test_size_new =test_size/(test_size + val_size)\n","    test_val_splits = test_val_splits['test'].train_test_split(test_size=test_size_new, seed=42, stratify_by_column='label')\n","    val_split = test_val_splits['train']\n","    test_split = test_val_splits['test']\n","\n","    train_val_dataset = DatasetDict({'train': train_split, 'val': val_split})\n","    test_dataset = DatasetDict({'test': test_split})\n","\n","    return train_val_dataset, test_dataset\n"]},{"cell_type":"markdown","id":"5ffc2f60","metadata":{"id":"5ffc2f60"},"source":["## Function to Create smaller subset"]},{"cell_type":"code","execution_count":null,"id":"b2d2507b","metadata":{"id":"b2d2507b"},"outputs":[],"source":["def get_small_balanced_subset(dataset, num_samples_per_class):\n","    subset = DatasetDict()\n","\n","    for split in list(dataset.keys()):\n","        texts = []\n","        labels = []\n","        for label in range(10):\n","            label_texts = dataset[split].filter(lambda x: x['label'] == label)['text']\n","            label_subset = random.sample(list(label_texts), num_samples_per_class)\n","            texts.extend(label_subset)\n","            labels.extend([label]*len(label_subset))\n","\n","        subset[split] = Dataset.from_dict({'text': texts, 'label': labels})\n","\n","    return subset"]},{"cell_type":"markdown","id":"9c9671ad","metadata":{"id":"9c9671ad"},"source":["## Function for Tokenization"]},{"cell_type":"code","execution_count":null,"id":"06ad039a","metadata":{"id":"06ad039a"},"outputs":[],"source":["def get_tokenized_dataset(checkpoint, dataset):\n","    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","    def tokenize_fn(batch):\n","        # Debug print statements\n","        # print(f\"Type of batch['text']: {type(batch['text'])}\")\n","        # print(f\"First item in batch['text']: {batch['text'][0]}\")\n","        return tokenizer(batch[\"text\"], truncation=True)\n","\n","    tokenized_dataset = dataset.map(tokenize_fn, batched=True)\n","    return tokenized_dataset\n"]},{"cell_type":"markdown","id":"9ea508bc","metadata":{"id":"9ea508bc"},"source":["## Function to Create Datasets"]},{"cell_type":"code","execution_count":null,"id":"6bffad28","metadata":{"id":"6bffad28"},"outputs":[],"source":["def setup_dataset(data_folder, class_names, num_samples_per_class):\n","\n","    # Constants for loading and splitting\n","    data_path = data_folder / 'multiclass_hw_basic_clean.csv'\n","    label_columns_name = 'Tag_Number_final'\n","    text_column_name = 'basic_cleaned_text'\n","\n","    # 1. Load Dataset\n","    dataset = load_custom_dataset(data_path, label_columns_name, text_column_name, class_names=class_names)\n","\n","    # 2. Split Dataset\n","    train_val_dataset, test_dataset = split_dataset(dataset, train_size=0.6, val_size=0.2, test_size=0.2)\n","\n","    # 3. Get Small Balanced Subset\n","    train_val_subset = get_small_balanced_subset(train_val_dataset, num_samples_per_class=num_samples_per_class)\n","\n","    return train_val_subset, train_val_dataset, test_dataset\n","\n"]},{"cell_type":"markdown","id":"0b4b6048","metadata":{"id":"0b4b6048"},"source":["## Function to Initialize Model"]},{"cell_type":"code","execution_count":null,"id":"0cac30bc","metadata":{"id":"0cac30bc"},"outputs":[],"source":["def initialize_model(checkpoint, class_names):\n","    config = AutoConfig.from_pretrained(checkpoint)\n","    id2label = {}\n","    for id_, label_ in enumerate(class_names):\n","        id2label[str(id_)] = label_\n","\n","    label2id = {}\n","    for id_, label_ in enumerate(class_names):\n","        label2id[label_] = id_\n","\n","    config.id2label = id2label\n","    config.label2id = label2id\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n","    return model"]},{"cell_type":"markdown","id":"79623f01","metadata":{"id":"79623f01"},"source":["## Function to Compute Metrics"]},{"cell_type":"code","execution_count":null,"id":"8fe967fb","metadata":{"id":"8fe967fb"},"outputs":[],"source":["def compute_metrics(eval_pred):\n","\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    f1_metric = evaluate.load(\"f1\", average=\"macro\")\n","    accuracy = evaluate.load(\"accuracy\")\n","\n","    evaluations = {}\n","    evaluations.update(f1_metric.compute(predictions=predictions, references = labels, average=\"macro\"))\n","    evaluations.update(accuracy.compute(predictions=predictions, references = labels))\n","\n","    return evaluations"]},{"cell_type":"markdown","id":"52370c8a","metadata":{"id":"52370c8a"},"source":["## Function to set Trainer"]},{"cell_type":"code","execution_count":null,"id":"4240b3e8","metadata":{"id":"4240b3e8"},"outputs":[],"source":["def get_trainer(model, training_args, train_dataset, eval_dataset, compute_metrics, tokenizer, data_collator):\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=eval_dataset,\n","        compute_metrics=compute_metrics,\n","        tokenizer=tokenizer,\n","        data_collator= data_collator\n","    )\n","    return trainer"]},{"cell_type":"markdown","id":"af37d2ae","metadata":{"id":"af37d2ae"},"source":["## Function to plot confusion matrix"]},{"cell_type":"code","execution_count":null,"id":"f9cad3f5","metadata":{"id":"f9cad3f5"},"outputs":[],"source":["def log_and_plot_confusion_matrix(trainer, tokenized_val_dataset, class_names):\n","    # Perform prediction using the trainer\n","    valid_output = trainer.predict(tokenized_val_dataset)\n","\n","    # Extract the predicted labels and true labels\n","    valid_preds = np.argmax(valid_output.predictions, axis=1)\n","    valid_labels = np.array(valid_output.label_ids)\n","\n","    # Log the confusion matrix to wandb\n","    wandb.log({\n","        \"conf_mat\": wandb.plot.confusion_matrix(\n","            preds=valid_preds,\n","            y_true=valid_labels,\n","            class_names=class_names\n","        )\n","    })\n","\n","    # Plot the confusion matrix using Matplotlib\n","    fig, ax = plt.subplots(figsize=(8, 6))\n","    ConfusionMatrixDisplay.from_predictions(\n","        y_true=valid_labels,\n","        y_pred=valid_preds,\n","        ax=ax,\n","        normalize=\"true\",\n","        display_labels=class_names,\n","        xticks_rotation=90\n","    )\n","    plt.show()"]},{"cell_type":"markdown","id":"4bf60cc4","metadata":{"id":"4bf60cc4"},"source":["## Function to free memory"]},{"cell_type":"code","execution_count":null,"id":"7200c892","metadata":{"id":"7200c892"},"outputs":[],"source":["def free_memory():\n","    \"\"\"\n","    Attempts to free up memory by deleting variables and running Python's garbage collector.\n","    \"\"\"\n","    gc.collect()\n","    for device_id in range(torch.cuda.device_count()):\n","        torch.cuda.set_device(device_id)\n","        torch.cuda.empty_cache()\n","    gc.collect()\n"]},{"cell_type":"markdown","id":"7cef1674","metadata":{"id":"7cef1674"},"source":["## Function to tokenize dataset and, train and eval models\n","\n","<font color = 'indianred'>**ALLOWED TO CHANGE THE BLOCK IN THE FUNCTION BELOW**"]},{"cell_type":"code","execution_count":null,"id":"8be2ae4f","metadata":{"id":"8be2ae4f"},"outputs":[],"source":["def tokenize_train_evaluate_log(training_args, checkpoint, base_folder,\n","                             class_names, train_val_subset, compute_metrics):\n","    # 1. Free memory\n","    free_memory()\n","\n","    # 2. Setup wandb\n","    wandb.login()\n","    %env WANDB_PROJECT = nlp_course_fall_2023-HW5-Part-B-Colab\n","\n","    ######################## ALLOWED TO CHANGE THIS BLOCK ################################################\n","\n","    # MAKE SURE THE BASE FOLDER IS SETUP CORRECTLY\n","    #  YOU CAN CHANGE THIS LINE IF YOU WANT TO SAVE IN A DIFFERENT FOLDER\n","\n","    model_folder = base_folder / \"models\" / \"nlp_fall_2023/stack\"/checkpoint\n","    model_folder.mkdir(exist_ok=True, parents=True)\n","\n","    ######################## ALLOWED TO CHANGE THIS BLOCK ################################################\n","\n","\n","    # 3. Get Tokenized Dataset and Data Collator\n","    train_val_tokenized_dataset = get_tokenized_dataset(checkpoint, train_val_subset)\n","\n","    # 4. Initialize Model and Tokenizer\n","    model = initialize_model(checkpoint, class_names)\n","    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","    # 5. Initialize Trainer\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","    trainer = get_trainer(model, training_args, train_val_tokenized_dataset['train'],\n","                          train_val_tokenized_dataset['val'], compute_metrics, tokenizer, data_collator)\n","\n","    # 6. Train and Evaluate\n","    trainer.train()\n","    trainer.evaluate(train_val_tokenized_dataset['val'])\n","\n","    # 7. Log Metrics and Plot\n","    log_and_plot_confusion_matrix(trainer, train_val_tokenized_dataset['val'], class_names)\n","\n","    best_model_checkpoint_step = trainer.state.best_model_checkpoint.split('-')[-1]\n","    wandb.log({\"best_model_checkpoint_step\": best_model_checkpoint_step})\n","    print(f\"The best model was saved at step {best_model_checkpoint_step}.\")\n","\n","    wandb.finish()"]},{"cell_type":"markdown","id":"648be75c","metadata":{"id":"648be75c"},"source":["## Initial Training Arguments\n","\n","DO NOT CHANGE ANY ARGUMENTS"]},{"cell_type":"code","execution_count":null,"id":"a81746c2","metadata":{"id":"a81746c2"},"outputs":[],"source":["training_args = TrainingArguments(\n","    # Training-specific configurations\n","    num_train_epochs=2,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    weight_decay=0.01,  # Apply L2 regularization to prevent overfitting\n","    learning_rate=2e-5,  # Step size for the optimizer during training\n","    optim='adamw_torch',  # Optimizer,\n","\n","    # Checkpoint saving and model evaluation settings\n","    output_dir=str(model_folder),  # Directory to save model checkpoints\n","    evaluation_strategy='steps',  # Evaluate model at specified step intervals\n","    eval_steps=20,  # Perform evaluation every 10 training steps\n","    save_strategy=\"steps\",  # Save model checkpoint at specified step intervals\n","    save_steps=20,  # Save a model checkpoint every 10 training steps\n","    load_best_model_at_end=True,  # Reload the best model at the end of training\n","    save_total_limit=2,  # Retain only the best and the most recent model checkpoints\n","    # Use 'accuracy' as the metric to determine the best model\n","    metric_for_best_model=\"accuracy\",\n","    greater_is_better=True,  # A model is 'better' if its accuracy is higher\n","    # Experiment logging configurations (commented out in this example)\n","    logging_strategy='steps',\n","    logging_steps=20,\n","    report_to='wandb',  # Log metrics and results to Weights & Biases platform\n","    run_name= 'exp1',  # Experiment name for Weights & Biases\n",")"]},{"cell_type":"markdown","id":"1e09e994","metadata":{"id":"1e09e994"},"source":["# Experiments"]},{"cell_type":"markdown","id":"9cf49386","metadata":{"id":"9cf49386"},"source":["## Dataset hyperparameters"]},{"cell_type":"code","execution_count":null,"id":"0e15f38c","metadata":{"id":"0e15f38c"},"outputs":[],"source":["class_names = ['c#', 'java', 'php','javascript', 'android', 'jquery', 'c++',  'python', 'iphone', 'asp.net']\n","num_samples_per_class = 200\n","train_val_subset, train_val_dataset, test_dataset = setup_dataset(data_folder, class_names, num_samples_per_class)"]},{"cell_type":"markdown","id":"7d562e17","metadata":{"id":"7d562e17"},"source":["## Experiment 1 : distilbert-base-uncased"]},{"cell_type":"markdown","id":"2784e7fe","metadata":{"id":"2784e7fe"},"source":["### Trainer hyperparameters"]},{"cell_type":"code","execution_count":null,"id":"59c49186","metadata":{"id":"59c49186"},"outputs":[],"source":["checkpoint = # CODE HERE\n","training_args_dict = training_args.to_dict() # Convert TrainingArguments to dictionary\n","training_args_dict['run_name'] = f'{checkpoint}-{num_samples_per_class}' # Update the run_name\n","new_training_args = TrainingArguments(**training_args_dict)"]},{"cell_type":"code","execution_count":null,"id":"bfb9a11b","metadata":{"id":"bfb9a11b"},"outputs":[],"source":["tokenize_train_evaluate_log(training_args= new_training_args, checkpoint=checkpoint, base_folder=base_folder,\n","                             class_names=class_names, train_val_subset=train_val_subset,\n","                             compute_metrics=compute_metrics)"]},{"cell_type":"markdown","id":"2d74ec70","metadata":{"id":"2d74ec70"},"source":["## Experiment 2 : Your choice of checkpoint"]},{"cell_type":"markdown","id":"71291f73","metadata":{"id":"71291f73"},"source":["### Trainer hyperparameters"]},{"cell_type":"code","execution_count":null,"id":"f7433c1c","metadata":{"id":"f7433c1c"},"outputs":[],"source":["checkpoint = # CODE HERE\n","training_args_dict = training_args.to_dict() # Convert TrainingArguments to dictionary\n","training_args_dict['run_name'] = f'{checkpoint}-{num_samples_per_class}' # Update the run_name\n","new_training_args = TrainingArguments(**training_args_dict)\n"]},{"cell_type":"code","execution_count":null,"id":"3c09cec3","metadata":{"id":"3c09cec3"},"outputs":[],"source":["tokenize_train_evaluate_log(training_args= new_training_args, checkpoint=checkpoint, base_folder=base_folder,\n","                             class_names=class_names, train_val_subset=train_val_subset,\n","                             compute_metrics=compute_metrics)"]},{"cell_type":"markdown","id":"f63f639b","metadata":{"id":"f63f639b"},"source":["## Experiment 3 -  Your second choice of checkpoint here"]},{"cell_type":"markdown","id":"bf2d4a3e","metadata":{"id":"bf2d4a3e"},"source":["### Trainer hyperparameters"]},{"cell_type":"code","execution_count":null,"id":"934b91bd","metadata":{"id":"934b91bd"},"outputs":[],"source":["checkpoint =  # CODE HERE\n","training_args_dict = training_args.to_dict() # Convert TrainingArguments to dictionary\n","training_args_dict['run_name'] = f'{checkpoint}-{num_samples_per_class}' # Update the run_name\n","new_training_args = TrainingArguments(**training_args_dict)\n"]},{"cell_type":"code","execution_count":null,"id":"64553710","metadata":{"id":"64553710"},"outputs":[],"source":["tokenize_train_evaluate_log(training_args= new_training_args, checkpoint=checkpoint, base_folder=base_folder,\n","                             class_names=class_names, train_val_subset=train_val_subset,\n","                             compute_metrics=compute_metrics)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":5}